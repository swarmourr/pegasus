{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Containers \n",
    "\n",
    "Application Containers such as Docker and Singularity are an attractive way to to package software with complex dependencies to be used during workflow execution. Use of containers ensures that your application code always sees the same runtime environment, when a job in your workflow runs on a remote worker node. It minimizes the chances of running into errors related to differing versions of dependant libraries installed on the worker nodes where the jobs of your workflow run. Additonally, use of containers promotes reproducibility as they provide a fully defined and reproducible environment in which your jobs run.\n",
    "\n",
    "This section of the tutorial will explain how you can specify a **Docker** container in Pegasus, to indicate the container in which the jobs of your workflow run.\n",
    "\n",
    "## Diamond Workflow with Containers\n",
    "\n",
    "This notebook will generate the **diamond workflow** that we used in Exercise 1. Instead of executing the executables directly on the worker node, we will specify a **base container image** in which the executables defined in the Transformation Catalog should execute in. \n",
    "\n",
    "![Diamond Workflow](../images/diamond.svg)\n",
    "\n",
    " \n",
    "## 1. Docker Container Notes\n",
    "\n",
    "The Docker image we use for this exercise is a minimal image based on Ubuntu bionic, and includes a python3 install in it.\n",
    "\n",
    "This image is approximately 100MB in size, and is well suited for use in a tutorial setting. The image can be found on DockerHub [here] (https://hub.docker.com/repository/docker/karanvahi/pegasus-tutorial-minimal).\n",
    "\n",
    "\n",
    "### Dockerfile\n",
    "\n",
    "Containers are accompanied by a recipe file that contains instructions on how to build the container. Using the container technology specific build commands, you can then build the container locally and even push to a remote repositing In case of Docker, this recipe file is called a ***Dockerfile*** and images are built using ***docker build*** command. \n",
    "\n",
    "The associated Dockerfile is shown below\n",
    "<br>\n",
    "```\n",
    "FROM ubuntu:bionic\n",
    "\n",
    "RUN groupadd --gid 808 scitech-group\n",
    "RUN useradd --gid 808 --uid 550 --create-home --password '$6$ouJkMasm5X8E4Aye$QTFH2cHk4b8/TmzAcCxbTz7Y84xyNFs.gqm/HWEykdngmOgELums1qOi3e6r8Z.j7GEA9bObS/2pTN1WArGNf0' scitech\n",
    "\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    python3 \\\n",
    "    wget \\\n",
    "    && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# CA certs\n",
    "RUN mkdir -p /etc/grid-security && \\\n",
    "    cd /etc/grid-security && \\\n",
    "    wget -nv --no-check-certificate https://download.pegasus.isi.edu/containers/certificates.tar.gz && \\\n",
    "    tar xzf certificates.tar.gz && \\\n",
    "    rm -f certificates.tar.gz\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The first line in the file containers the FROM instruction, which specifies the Base image from which we are building our container image. In this case, we are basing our image on a public Ubuntu bionic image. The subsequent RUN commands in the file indicate the steps/commands to execute to install software, additional libraries etc. Specifically for this image we do\n",
    "\n",
    "- add a user scitech to the image.\n",
    "- install python3 that is required \n",
    "- wget a command line tool to retrieve data over http\n",
    "- setup CA certificates in the container image\n",
    " \n",
    "### Building your own container\n",
    "\n",
    "For this exercise, the container is already built and availble on DockerHub. However, we do include the Dockerfile in this folder, for you to build your own container. \n",
    "\n",
    "```bash\n",
    "$ docker build --tag <username>/pegasus-tutorial-minimal -f Dockerfile.minimal .\n",
    "\n",
    "[+] Building 1.5s (11/11) FINISHED                                                                                                          \n",
    " => [internal] load build definition from Dockerfile.minimal    0.0s\n",
    " => => transferring dockerfile: 46B                             0.0s\n",
    " => [internal] load .dockerignore                               0.0s\n",
    " => => transferring context: 2B                                 0.0s\n",
    " => [internal] load metadata for docker.io/library/ubuntu:bionic 1.4s\n",
    " => [1/7] FROM docker.io/library /ubuntu:bionic@sha256:478caf1bec1afd54a58435ec681c8755883b7eb843a8630091890130b15a79af                 0.0s\n",
    " => CACHED [2/7] RUN groupadd --gid 808 scitech-group            0.0s\n",
    " => CACHED [3/7] RUN useradd --gid 808 --uid 550 --create-home --password 'xxx' 0.0s\n",
    " => CACHED [4/7] RUN apt-get update && apt-get install -y --no-install-recommends     python3     wget     &&     apt-get clean &&     0.0s\n",
    " => CACHED [5/7] RUN mkdir -p /etc/grid-security &&     cd /etc/grid-security &&     wget -nv --no-check-certificate https://download  0.0s\n",
    " => CACHED [6/7] RUN echo -e \"scitech ALL=(ALL)       NOPASSWD:ALL\\n\" >> /etc/sudoers      0.0s\n",
    " => exporting to image       0.0s\n",
    " => => exporting layers        0.0s\n",
    " => => writing image sha256:302647114737b17e2d6cc4edb542542f0dca27b5a810c6da957e442e7bb94332                                           0.0s\n",
    " => => naming to docker.io/<username>/pegasus-tutorial-minimal                  \n",
    "```\n",
    "\n",
    "In the above command replace <username> with your DockerHub user name.\n",
    "    \n",
    "After running the above command, you have the container locally built on your machine. You can check the size of the image by running the following command\n",
    "    \n",
    "```bash\n",
    "$  docker image ls <username>i/pegasus-tutorial-minimal  \n",
    "REPOSITORY                           TAG       IMAGE ID       CREATED        SIZE\n",
    "<username>/pegasus-tutorial-minimal   latest    302647114737   24 hours ago   94.6MB    \n",
    "```\n",
    "\n",
    "### Pushing your container to Docker Hub\n",
    "\n",
    "Docker Hub is an online repository where users push and share their container images. You can specify container images in a Docker Hub for your workflow to use for running jobs. Below is a brief series of steps that you need to do to push your built image to a repostory. Complete instructions can be found [here](https://docs.docker.com/docker-hub/repos/). \n",
    "\n",
    "1. Create a repository with the same name in the web interface by logging on to Docker Hub. In this case, you will create an empty public repository with the name ***pegasus-tutorial-minimal***.\n",
    "\n",
    "2. Login to Docker Hub on the command line\n",
    "  ```bash\n",
    "  $ docker login\n",
    "    Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\n",
    "    Username: <username>   \n",
    "    Password: \n",
    "    Login Succeeded\n",
    "  ```\n",
    "    \n",
    "3. Once logged in you can push the locally built container using docker push command\n",
    "    ```bash\n",
    "    $ docker push <username>/pegasus-tutorial-minimal\n",
    "    Using default tag: latest\n",
    "The push refers to repository [docker.io/karanvahi/pegasus-tutorial-minimal]\n",
    "5f70bf18a086: Mounted from localstack/localstack \n",
    "57efc43f999d: Pushed \n",
    "49a250fa7278: Pushed \n",
    "e78401c15cc7: Pushed \n",
    "137e6caf5967: Pushed \n",
    "5edeecf7c3a8: Pushed \n",
    "95129a5fe07e: Mounted from library/ubuntu \n",
    "latest: digest: sha256:4ee35dc2b527759d574d36339f5bb8fa26fb57d31213de0ac01cad56b8a9b444 size: 1779\n",
    "\n",
    "    ```\n",
    " \n",
    "**Note:** In the above commands replace <username> with your Docker Hub username."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Python API, Setup Logging and  Replica Catalog\n",
    "\n",
    "The steps to import the python API, setup logging and configuring the Replica Catalog is exactly the same as Exercise 1. We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"                                                                    \n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props.write() # written to ./pegasus.properties \n",
    "\n",
    "with open(\"f.a\", \"w\") as f:\n",
    "    f.write(\"This is the contents of the input file for the diamond workflow!\")\n",
    "    \n",
    "# --- Replicas -----------------------------------------------------------------\n",
    "fa = File(\"f.a\").add_metadata(creator=\"ryan\")\n",
    "rc = ReplicaCatalog()\\\n",
    "    .add_replica(\"local\", fa, Path(\".\").resolve() / \"f.a\")\\\n",
    "    .write() # written to ./replicas.yml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets crosscheck to see if the Replica Catalog file is created correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat replicas.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformation Catalog: Specify the Container for the jobs\n",
    "\n",
    "Users have the option of either using a different container for each executable or same container for all executables. When using containers with Pegasus you have two options\n",
    "\n",
    "1. The container has your executables pre installed. In that case in your transformation catalog, you specify the PFN as the path in the container where your executable is accessible\n",
    "\n",
    "2. The other case, is you are using a generic baseline container and want to let Pegasus stage your executables in at runtime. To do that you can mark the executable as **stageable** (is_stageable as True) and Pegasus will stage the executable into the container, as part of executable staging.\n",
    "\n",
    "In the example below, we are indicating that the preprocess, findrange and analyze executables need a container named *base_container* to run. However, we are going to let Pegasus stage them into container when your workflow runs from their location on site `condorpool` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Container ----------------------------------------------------------\n",
    "\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.DOCKER,\n",
    "                  image=\"docker://karanvahi/pegasus-tutorial-minimal\"\n",
    "    \n",
    "                  # comment out the location below (and comment the above location) \n",
    "                  # if you run into docker rate pull limits. Do this if your  \n",
    "                  # workflow fails on the first try with stage-in jobs fail \n",
    "                  # with error like ERROR: toomanyrequests: Too Many Requests. OR\n",
    "                  # You have reached your pull rate limit. You may increase \n",
    "                  # the limit by authenticating and upgrading: \n",
    "                  # ttps://www.docker.com/increase-rate-limits. \n",
    "                  # You must authenticate your pull requests.\n",
    "                  #\n",
    "                  # This is why Pegasus supports tar files of containers, \n",
    "                  # and also ensures the pull from a docker hub happens only \n",
    "                  # once per workflow\n",
    "    \n",
    "                  #image=\"http://download.pegasus.isi.edu/pegasus/tutorial/pegasus-tutorial-minimal.tar.gz\"\n",
    "               )\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "findrange = Transformation(\n",
    "                \"findrange\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "analyze = Transformation(\n",
    "                \"analyze\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(preprocess, findrange, analyze)\\\n",
    "    .write() # ./written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the container is listed once, and multiple transformations can refer to the same container.\n",
    "\n",
    "Some attributes to keep an eye out for\n",
    "- *name*  the name assigned to the container that is used as a reference handle when describing executables in Transformation\n",
    "\n",
    "- *type*  type of Container. Usually is Dokcer or Singularity\n",
    "\n",
    "- *image* - URL to image in a docker|singularity hub or URL to an existing docker image exported as a tar file or singularity image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"blackdiamond\")\n",
    "\n",
    "fb1 = File(\"f.b1\")\n",
    "fb2 = File(\"f.b2\")\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(\"-a\", \"preprocess\", \"-T\", \"3\", \"-i\", fa, \"-o\", fb1, fb2)\\\n",
    "                    .add_inputs(fa)\\\n",
    "                    .add_outputs(fb1, fb2)\n",
    "\n",
    "fc1 = File(\"f.c1\")\n",
    "job_findrange_1 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb1, \"-o\", fc1)\\\n",
    "                    .add_inputs(fb1)\\\n",
    "                    .add_outputs(fc1)\n",
    "\n",
    "fc2 = File(\"f.c2\")\n",
    "job_findrange_2 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb2, \"-o\", fc2)\\\n",
    "                    .add_inputs(fb2)\\\n",
    "                    .add_outputs(fc2)\n",
    "\n",
    "fd = File(\"f.d\")\n",
    "job_analyze = Job(analyze)\\\n",
    "                .add_args(\"-a\", \"analyze\", \"-T\", \"3\", \"-i\", fc1, fc2, \"-o\", fd)\\\n",
    "                .add_inputs(fc1, fc2)\\\n",
    "                .add_outputs(fd)\n",
    "\n",
    "wf.add_jobs(job_preprocess, job_findrange_1, job_findrange_2, job_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Workflow\n",
    "\n",
    "When working in Python, we can just use the reference do the `Workflow` object, you can plan, run, and monitor the workflow directly. These are wrappers around Pegasus CLI tools, and as such, the same arguments may be passed to them. \n",
    "\n",
    "**Note that the Pegasus binaries must be added to your PATH for this to work.**\n",
    "\n",
    "Please wait for the progress bar to indicate that the workflow has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object.\n",
    "\n",
    "## 6. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Container Setup on a Worker Node\n",
    "\n",
    "Now that we have been able to run the workflow succesfully, lets look beneath the covers to see how a job that has to run in a container gets setup on a worker node. The container setup for a job happens within PegasusLite, a light-weight Pegasus remote execution engine which wraps the user task on the remote worker node when a job is scheduled to the node. \n",
    "\n",
    "PegasusLite is responsible for figuring out the appropriate job directory in which the job executes, staging-in datasets that a job requires, launching the job, staging-out data, and cleaning up the job directory.\n",
    "\n",
    "![Container Setup in PegasusLite](../images/container-host.png)\n",
    "\n",
    "To see how Pegasus handled the container in this case, let’s look at some plumbing for one of the `analyze` job. The HTCondor submit file can be seen with:\n",
    "\n",
    "```bash\n",
    "$ cat `find scitech/pegasus/blackdiamond/run0001 -name analyze_ID0000004.sub`\n",
    "```\n",
    "\n",
    "Look  at the transfer_input_files attribute line, and specifically for the `base-container` file. It is transferred together with all the other inputs for the job.\n",
    "\n",
    "\n",
    "transfer_input_files = analyze,f.c2,f.c1,**base-container**,..\n",
    "\n",
    "Looking at the corresponding .sh file we can see how Pegasus executed the container by invoking `docker run` on a script written out at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat `find scitech/pegasus/blackdiamond/run0001 -name analyze_ID0000004.sub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What's Next?\n",
    "\n",
    "Next Notebook is `05-Summary`, that summarizes what we have learnt so far.\n",
    "To continue learning about advanced topics please open the notebook in `06-Advanced-Topics/` to learn about application checkpointing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
